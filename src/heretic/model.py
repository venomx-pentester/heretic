# SPDX-License-Identifier: AGPL-3.0-or-later
# Copyright (C) 2025-2026  Philipp Emanuel Weidmann <pew@worldwidemann.com> + contributors

import math
import subprocess
import sys
from contextlib import suppress
from dataclasses import dataclass
from typing import Any, Type, cast

import bitsandbytes as bnb
import questionary
import torch
import torch.linalg as LA
import torch.nn.functional as F
from peft import LoraConfig, PeftModel, get_peft_model
from peft.tuners.lora.layer import Linear
from torch import LongTensor, Tensor
from torch.nn import Module, ModuleList
from transformers import (
    AutoModelForCausalLM,
    AutoModelForImageTextToText,
    AutoTokenizer,
    BatchEncoding,
    BitsAndBytesConfig,
    PretrainedConfig,
    PreTrainedModel,
    PreTrainedTokenizerBase,
    TextStreamer,
)
from transformers.generation import (
    GenerateDecoderOnlyOutput,  # ty:ignore[possibly-missing-import]
)

from .config import QuantizationMethod, RowNormalization, Settings
from .utils import Prompt, batchify, empty_cache, print


def get_model_class(
    model: str,
) -> Type[AutoModelForImageTextToText] | Type[AutoModelForCausalLM]:
    configs = PretrainedConfig.get_config_dict(model)

    if any([("vision_config" in config) for config in configs]):
        return AutoModelForImageTextToText
    else:
        return AutoModelForCausalLM


@dataclass
class AbliterationParameters:
    max_weight: float
    max_weight_position: float
    min_weight: float
    min_weight_distance: float


class Model:
    model: PreTrainedModel | PeftModel
    tokenizer: PreTrainedTokenizerBase
    peft_config: LoraConfig

    def __init__(self, settings: Settings):
        self.settings = settings
        self.response_prefix = ""
        self.needs_reload = False

        print()
        print(f"Loading model [bold]{settings.model}[/]...")

        self.tokenizer = AutoTokenizer.from_pretrained(
            settings.model,
            trust_remote_code=settings.trust_remote_code,
        )

        # Fallback for tokenizers that don't declare a special pad token.
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # CRITICAL: Always use left-padding for decoder-only models during generation.
        #           Right-padding causes empty outputs because the model sees PAD tokens
        #           after the prompt and thinks the sequence is complete.
        self.tokenizer.padding_side = "left"

        self.model = None  # ty:ignore[invalid-assignment]
        self.max_memory = (
            {int(k) if k.isdigit() else k: v for k, v in settings.max_memory.items()}
            if settings.max_memory
            else None
        )
        self.device_map: str | dict = settings.device_map
        self.trusted_models = {settings.model: settings.trust_remote_code}

        if self.settings.evaluate_model is not None:
            self.trusted_models[settings.evaluate_model] = settings.trust_remote_code

        abort = False
        _vram_calibrated = False  # True after one post-load calibration pass.
        for dtype in settings.dtypes:
            if abort:
                break
            print(f"* Trying dtype [bold]{dtype}[/]... ", end="")

            while True:
                try:
                    quantization_config = self._get_quantization_config(dtype)

                    extra_kwargs = {}
                    # Only include quantization_config if it's not None
                    # (some models like gpt-oss have issues with explicit None).
                    if quantization_config is not None:
                        extra_kwargs["quantization_config"] = quantization_config

                    # Pass trust_remote_code=False (not None) when trust hasn't been
                    # established yet. This prevents HF from showing its own interactive
                    # prompt; we handle that ourselves below with clearer context.
                    self.model = get_model_class(settings.model).from_pretrained(
                        settings.model,
                        dtype=dtype,
                        device_map=self.device_map,
                        max_memory=self.max_memory,
                        trust_remote_code=self.trusted_models.get(settings.model)
                        or False,
                        **extra_kwargs,
                    )

                    # If we reach this point and the model requires trust_remote_code,
                    # either the user accepted, or settings.trust_remote_code is True.
                    if self.trusted_models.get(settings.model) is None:
                        self.trusted_models[settings.model] = True

                    # A test run can reveal dtype-related problems such as the infamous
                    # "RuntimeError: probability tensor contains either `inf`, `nan` or element < 0"
                    # (https://github.com/meta-llama/llama/issues/380).
                    self.generate(
                        [
                            Prompt(
                                system=settings.system_prompt,
                                user="What is 1+1?",
                            )
                        ],
                        max_new_tokens=1,
                    )

                    # After a successful load and warmup on multi-GPU systems, check
                    # whether each GPU has enough free VRAM for batch inference. If not,
                    # compute corrected per-GPU caps from the actual measured allocations
                    # and reload once. This handles architectures (e.g. NemotronH) where
                    # SSM workspace and other one-time allocations during the first
                    # forward pass leave insufficient headroom for batched inference.
                    # Only applies to hybrid SSM models — regular transformers don't
                    # allocate persistent inference workspace on top of model weights.
                    if (
                        not _vram_calibrated
                        and torch.cuda.is_available()
                        and torch.cuda.device_count() > 1
                        and self._has_mamba_layers()
                    ):
                        _HEADROOM = 6 * 1024**3  # 6 GiB minimum free per GPU
                        gpu_count = torch.cuda.device_count()
                        min_free = min(
                            torch.cuda.mem_get_info(i)[0] for i in range(gpu_count)
                        )
                        # Skip calibration if the model was disk-offloaded — this
                        # means total VRAM is insufficient, and rebalancing GPU caps
                        # cannot fix a capacity problem, only a distribution problem.
                        disk_offloaded = any(
                            p.device.type == "meta" for p in self.model.parameters()
                        )
                        if min_free < _HEADROOM and not disk_offloaded:
                            print()
                            print(
                                f"[yellow]Only {min_free / (1024**3):.1f} GiB free on "
                                "most-loaded GPU — recalibrating layout for batch inference...[/]"
                            )
                            # Identify overloaded GPUs before releasing the model.
                            overloaded = {
                                i
                                for i in range(gpu_count)
                                if torch.cuda.mem_get_info(i)[0] < _HEADROOM
                            }

                            # Release model so we can measure true available VRAM.
                            self.model = None  # ty:ignore[invalid-assignment]
                            empty_cache()

                            max_mem: dict[int | str, str] = {}
                            for i in range(gpu_count):
                                free_i, _ = torch.cuda.mem_get_info(i)
                                # Reserve headroom for inference working memory
                                # (SSM workspace, KV cache, activations, etc.).
                                usable = max(free_i - _HEADROOM, 0)
                                if i in overloaded:
                                    # Apply correction to prevent Accelerate from
                                    # overloading this GPU again due to layer-size
                                    # underestimation (~30% on hybrid architectures).
                                    stated_gib = max(int(usable / (1024**3) * 0.7), 1)
                                else:
                                    # Full budget — this GPU absorbs displaced layers.
                                    stated_gib = max(int(usable / (1024**3)), 1)
                                max_mem[i] = f"{stated_gib}GiB"
                            caps = ", ".join(
                                f"GPU {k}: {v}" for k, v in max_mem.items()
                            )
                            print(f"  [dim]Corrected caps: {caps}[/]")
                            self.max_memory = max_mem
                            _vram_calibrated = True
                            print(
                                f"* Retrying dtype [bold]{dtype}[/] with corrected caps... ",
                                end="",
                            )
                            continue  # reload this dtype with corrected max_memory
                except Exception as error:
                    self.model = None  # ty:ignore[invalid-assignment]
                    empty_cache()
                    print(f"[red]Failed[/] ({error})")

                    error_str = str(error).lower()

                    if "trust_remote_code" in error_str:
                        if self.trusted_models.get(settings.model) is None:
                            # Model requires custom code — explain and ask once.
                            print()
                            print(
                                "[yellow](This is expected — the model requires permission to run custom code.)[/]"
                            )
                            print(
                                f"[yellow][bold]{settings.model}[/bold] ships custom architecture "
                                "code that must be executed to load this model. "
                                f"You can inspect the repository at "
                                f"https://huggingface.co/{settings.model}[/]"
                            )
                            print()
                            if questionary.confirm(
                                "Trust and run this model's custom code?",
                                default=True,
                            ).ask():
                                self.trusted_models[settings.model] = True
                                print(f"* Retrying dtype [bold]{dtype}[/]... ", end="")
                                continue  # retry this dtype with trust granted
                            else:
                                self.trusted_models[settings.model] = False
                                abort = True
                        break  # trust already decided; move to next dtype or abort

                    if "mamba-ssm" in error_str:
                        # Missing dependency — retrying other dtypes won't help.
                        print()
                        print(
                            f"[bold red]mamba-ssm is required to load [cyan]{settings.model}[/cyan].[/]"
                        )
                        print()
                        if questionary.confirm(
                            "Install mamba-ssm now? (this may take several minutes)",
                            default=True,
                        ).ask():
                            try:
                                subprocess.check_call(
                                    [
                                        sys.executable,
                                        "-m",
                                        "pip",
                                        "install",
                                        "mamba-ssm",
                                    ]
                                )
                            except subprocess.CalledProcessError:
                                print()
                                print("[bold red]Auto-install failed.[/]")
                                print(
                                    "[yellow]mamba-ssm requires the CUDA toolkit (nvcc) to build. "
                                    "Install nvcc, then run:[/] pip install mamba-ssm"
                                )
                                print("[yellow]To install nvcc:[/]")
                                print(
                                    "  sudo apt install nvidia-cuda-toolkit   [dim]# Ubuntu/Debian[/]"
                                )
                                print(
                                    "  conda install -c nvidia cuda-nvcc       [dim]# Conda[/]"
                                )
                                raise SystemExit(1)
                            print()
                            print(
                                "[green]Installation complete. Retrying model load...[/]"
                            )
                            print()
                            continue  # retry this dtype after install
                        abort = True
                        break

                    # For all other errors, update trust cache if needed and try next dtype.
                    if self.trusted_models.get(settings.model) is None:
                        self.trusted_models[settings.model] = True
                    break
                else:
                    # Load and test generate succeeded — exit the retry loop.
                    break

            if abort or self.model is None:
                continue

            if settings.quantization == QuantizationMethod.BNB_4BIT:
                print("[green]Ok[/] (quantized to 4-bit precision)")
            else:
                print("[green]Ok[/]")

            break

        if self.model is None:
            raise Exception("Failed to load model with all configured dtypes.")

        self._apply_lora()

        # LoRA B matrices are initialized to zero by default in PEFT,
        # so we don't need to do anything manually.

        print(f"* Transformer model with [bold]{len(self.get_layers())}[/] layers")
        print("* Abliterable components:")
        components = self.get_abliterable_components()
        for component in components:
            # Count how many layers contain this component.
            layer_count = sum(
                1
                for i in range(len(self.get_layers()))
                if component in self.get_layer_modules(i)
            )
            print(f"  * [bold]{component}[/]: present in [bold]{layer_count}[/] layers")

        # If the model has Mamba/SSM layers, suggest installing the fast kernels.
        if any(c.startswith("mamba.") for c in components):
            try:
                import causal_conv1d  # ty:ignore[unresolved-import]  # noqa: F401
                import mamba_ssm  # ty:ignore[unresolved-import]  # noqa: F401
            except ImportError:
                print()
                print(
                    "[yellow]This hybrid model has Mamba/SSM layers. "
                    "For significantly faster inference, install the optimized kernels:[/]"
                )
                print("[yellow]  pip install causal-conv1d mamba-ssm[/]")
                print(
                    "[yellow]  (requires CUDA toolkit ≥ 11.6 — check with nvcc -V; "
                    "build takes ~10 min)[/]"
                )

    def _apply_lora(self):
        # Guard against calling this method at the wrong time.
        assert isinstance(self.model, PreTrainedModel)

        # Always use LoRA adapters for abliteration (faster reload, no weight modification).
        # We use the leaf names (e.g. "o_proj") as target modules.
        # This may cause LoRA adapters to be attached to unrelated modules (e.g. "conv.o_proj"),
        # but this is harmless as we only abliterate the modules we target in `abliterate()`,
        # leaving the others at their default (identity) state.
        # NOTE: This will need to be updated when hybrid layer support (#43) is merged.
        target_modules = [
            comp.split(".")[-1] for comp in self.get_abliterable_components()
        ]

        if self.settings.row_normalization != RowNormalization.FULL:
            # Rank 1 is sufficient for directional ablation without renormalization.
            lora_rank = 1
        else:
            # Row magnitude preservation introduces nonlinear effects.
            lora_rank = self.settings.full_normalization_lora_rank

        self.peft_config = LoraConfig(
            r=lora_rank,
            target_modules=target_modules,
            lora_alpha=lora_rank,  # Apply adapter at full strength.
            lora_dropout=0,
            bias="none",
            # Even if we're using AutoModelForImageTextToText, this is still correct,
            # as VL models are typically just causal LMs with an added image encoder.
            task_type="CAUSAL_LM",
        )

        # self.peft_config is a LoraConfig object rather than a dictionary,
        # so the result is a PeftModel rather than a PeftMixedModel.
        self.model = cast(PeftModel, get_peft_model(self.model, self.peft_config))

        print(f"* LoRA adapters initialized (targets: {', '.join(target_modules)})")

    def _get_quantization_config(self, dtype: str) -> BitsAndBytesConfig | None:
        """
        Creates quantization config based on settings.

        Args:
            dtype: The dtype string (e.g., "auto", "bfloat16")

        Returns:
            BitsAndBytesConfig or None
        """
        if self.settings.quantization == QuantizationMethod.BNB_4BIT:
            # BitsAndBytesConfig expects a torch.dtype, not a string.
            if dtype == "auto":
                compute_dtype = torch.bfloat16
            else:
                compute_dtype = getattr(torch, dtype)

            return BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=compute_dtype,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
            )
        return None

    def get_merged_model(self) -> PreTrainedModel:
        # Guard against calling this method at the wrong time.
        assert isinstance(self.model, PeftModel)

        # Check if we need special handling for quantized models.
        # This covers both user-applied BNB quantization and models with a built-in
        # quantization_config (FP8, MXFP4, etc.), since LoRA cannot be merged directly
        # into quantized weights — the base model must be reloaded in full precision first.
        pre_quantized = (
            getattr(self.model.config, "quantization_config", None) is not None
            and self.settings.quantization == QuantizationMethod.NONE
        )
        if self.settings.quantization == QuantizationMethod.BNB_4BIT or pre_quantized:
            # Quantized models need special handling - we must reload the base model
            # in full precision to merge the LoRA adapters

            # Get the adapter state dict before we do anything
            adapter_state = {}
            for name, param in self.model.named_parameters():
                if "lora_" in name:
                    adapter_state[name] = param.data.clone().cpu()

            # Load base model in full precision on CPU to avoid VRAM issues
            print("* Loading base model on CPU (this may take a while)...")
            base_model = get_model_class(self.settings.model).from_pretrained(
                self.settings.model,
                torch_dtype=self.model.dtype,
                device_map="cpu",
                trust_remote_code=self.trusted_models.get(self.settings.model),
            )

            # Apply LoRA adapters to the CPU model
            print("* Applying LoRA adapters...")
            peft_model = get_peft_model(base_model, self.peft_config)

            # Copy the trained adapter weights
            for name, param in peft_model.named_parameters():
                if name in adapter_state:
                    param.data = adapter_state[name].to(param.device)

            # Merge and unload
            print("* Merging LoRA adapters into base model...")
            merged_model = peft_model.merge_and_unload()
            return merged_model
        else:
            # Non-quantized model - can merge directly
            print("* Merging LoRA adapters into base model...")
            merged_model = self.model.merge_and_unload()
            # merge_and_unload() modifies self.model in-place, destroying LoRA adapters.
            # Mark for full reload if user switches trials later.
            self.needs_reload = True
            return merged_model

    def reset_model(self):
        """
        Resets the model to a clean state for the next trial or evaluation.

        Behavior:
        - Fast path: If the same model is loaded and doesn't need full reload,
          resets LoRA adapter weights to zero (identity transformation).
        - Slow path: If switching models or after merge_and_unload(),
          performs full model reload with quantization config.
        """
        current_model = getattr(self.model.config, "name_or_path", None)
        if current_model == self.settings.model and not self.needs_reload:
            # Reset LoRA adapters to zero (identity transformation)
            for name, module in self.model.named_modules():
                if "lora_B" in name and hasattr(module, "weight"):
                    torch.nn.init.zeros_(module.weight)
            return

        dtype = self.model.dtype

        # Purge existing model object from memory to make space.
        self.model = None  # ty:ignore[invalid-assignment]
        empty_cache()

        quantization_config = self._get_quantization_config(str(dtype).split(".")[-1])

        # Build kwargs, only include quantization_config if it's not None
        extra_kwargs = {}
        if quantization_config is not None:
            extra_kwargs["quantization_config"] = quantization_config

        self.model = get_model_class(self.settings.model).from_pretrained(
            self.settings.model,
            dtype=dtype,
            device_map=self.device_map,
            max_memory=self.max_memory,
            trust_remote_code=self.trusted_models.get(self.settings.model),
            **extra_kwargs,
        )

        self._apply_lora()

        self.needs_reload = False

    def _has_mamba_layers(self) -> bool:
        """Returns True if any layer has a Mamba/SSM out_proj (hybrid architectures)."""
        try:
            for layer in self.get_layers():
                if hasattr(layer, "mixer") and hasattr(layer.mixer, "out_proj"):
                    return True
        except Exception:
            pass
        return False

    def get_layers(self) -> ModuleList:
        model = self.model

        # Unwrap PeftModel (always true after _apply_lora)
        if isinstance(model, PeftModel):
            model = model.base_model.model

        # Most multimodal models.
        with suppress(Exception):
            return model.model.language_model.layers

        # NemotronH and other backbone-based models.
        with suppress(Exception):
            return model.backbone.layers

        # Text-only models.
        return model.model.layers

    def get_layer_modules(self, layer_index: int) -> dict[str, list[Module]]:
        layer = self.get_layers()[layer_index]

        modules = {}

        def try_add(component: str, module: Any):
            # Only add if it's a proper nn.Module (PEFT can wrap these with LoRA)
            if isinstance(module, Module):
                if component not in modules:
                    modules[component] = []
                modules[component].append(module)
            else:
                # Assert for unexpected types (catches architecture changes)
                assert not isinstance(module, Tensor), (
                    f"Unexpected Tensor in {component} - expected nn.Module"
                )

        # Standard transformer attention out-projection.
        with suppress(Exception):
            try_add("attn.o_proj", layer.self_attn.o_proj)  # ty:ignore[possibly-missing-attribute]

        # Most dense models.
        with suppress(Exception):
            try_add("mlp.down_proj", layer.mlp.down_proj)  # ty:ignore[possibly-missing-attribute]

        # Some MoE models (e.g. Qwen3).
        with suppress(Exception):
            for expert in layer.mlp.experts:  # ty:ignore[possibly-missing-attribute, not-iterable]
                try_add("mlp.down_proj", expert.down_proj)  # ty:ignore[possibly-missing-attribute]

        # Phi-3.5-MoE (and possibly others).
        with suppress(Exception):
            for expert in layer.block_sparse_moe.experts:  # ty:ignore[possibly-missing-attribute, not-iterable]
                try_add("mlp.down_proj", expert.w2)  # ty:ignore[possibly-missing-attribute]

        # Granite MoE Hybrid - attention layers with shared_mlp.
        with suppress(Exception):
            try_add("mlp.down_proj", layer.shared_mlp.output_linear)  # ty:ignore[possibly-missing-attribute]

        # Granite MoE Hybrid - MoE layers with experts.
        with suppress(Exception):
            for expert in layer.moe.experts:  # ty:ignore[possibly-missing-attribute, not-iterable]
                try_add("mlp.down_proj", expert.output_linear)  # ty:ignore[possibly-missing-attribute]

        # NemotronH hybrid layers - all use a unified `mixer` attribute.
        # Attention layers have mixer.o_proj.
        with suppress(Exception):
            try_add("attn.o_proj", layer.mixer.o_proj)  # ty:ignore[possibly-missing-attribute]

        # NemotronH simple MLP layers have mixer.down_proj.
        with suppress(Exception):
            try_add("mlp.down_proj", layer.mixer.down_proj)  # ty:ignore[possibly-missing-attribute]

        # NemotronH MoE layers have mixer.experts (per-expert) and mixer.shared_experts.
        # Following heretic's standard pattern for MoE models (Qwen3, Phi-3.5-MoE, Granite):
        # include all expert down_proj modules. Optuna will optimize the weight.
        with suppress(Exception):
            for expert in layer.mixer.experts:  # ty:ignore[possibly-missing-attribute, not-iterable]
                try_add("mlp.down_proj", expert.down_proj)  # ty:ignore[possibly-missing-attribute]

        with suppress(Exception):
            try_add("mlp.down_proj", layer.mixer.shared_experts.down_proj)  # ty:ignore[possibly-missing-attribute]

        # NemotronH Mamba2 SSM layers have mixer.out_proj.
        with suppress(Exception):
            try_add("mamba.out_proj", layer.mixer.out_proj)  # ty:ignore[possibly-missing-attribute]

        return modules

    def get_abliterable_components(self) -> list[str]:
        # Scan all layers to collect the union of component types.
        # This is necessary for hybrid architectures (e.g. NemotronH) where
        # different layers have different component types.
        all_components: dict[str, list[Module]] = {}
        n_layers = len(self.get_layers())
        skipped_layers: list[int] = []
        for layer_index in range(n_layers):
            layer_modules = self.get_layer_modules(layer_index)
            if not layer_modules:
                skipped_layers.append(layer_index)
                continue
            for component, modules in layer_modules.items():
                if component not in all_components:
                    all_components[component] = modules

        if skipped_layers:
            # Log which layers were skipped and what their structure looks like
            # so users can report the architecture for future support.
            sample_idx = skipped_layers[0]
            sample_layer = self.get_layers()[sample_idx]
            child_names = [name for name, _ in sample_layer.named_children()]
            print(
                f"  [yellow]Warning: {len(skipped_layers)}/{n_layers} layers have "
                f"no recognized abliterable modules (e.g. layer {sample_idx}: "
                f"{type(sample_layer).__name__} with children: {child_names})[/]"
            )

        assert len(all_components) > 0, (
            "No abliterable modules found in any layer. "
            "This model architecture may not be supported."
        )

        return list(all_components.keys())

    def abliterate(
        self,
        refusal_directions: Tensor,
        direction_index: float | None,
        parameters: dict[str, AbliterationParameters],
    ):
        if direction_index is None:
            refusal_direction = None
        else:
            # The index must be shifted by 1 because the first element
            # of refusal_directions is the direction for the embeddings.
            weight, index = math.modf(direction_index + 1)
            refusal_direction = F.normalize(
                refusal_directions[int(index)].lerp(
                    refusal_directions[int(index) + 1],
                    weight,
                ),
                p=2,
                dim=0,
            )

        # Note that some implementations of abliteration also orthogonalize
        # the embedding matrix, but it's unclear if that has any benefits.
        for layer_index in range(len(self.get_layers())):
            for component, modules in self.get_layer_modules(layer_index).items():
                params = parameters[component]

                # Type inference fails here for some reason.
                distance = cast(float, abs(layer_index - params.max_weight_position))

                # Don't orthogonalize layers that are more than
                # min_weight_distance away from max_weight_position.
                if distance > params.min_weight_distance:
                    continue

                # Interpolate linearly between max_weight and min_weight
                # over min_weight_distance.
                weight = params.max_weight + (distance / params.min_weight_distance) * (
                    params.min_weight - params.max_weight
                )

                if refusal_direction is None:
                    # The index must be shifted by 1 because the first element
                    # of refusal_directions is the direction for the embeddings.
                    layer_refusal_direction = refusal_directions[layer_index + 1]
                else:
                    layer_refusal_direction = refusal_direction

                for module in modules:
                    # FIXME: This cast is potentially invalid, because the program logic
                    #        does not guarantee that the module is of type Linear, and in fact
                    #        the retrieved modules might not conform to the interface assumed
                    #        below (though they do in practice). However, this is difficult
                    #        to fix cleanly, because get_layer_modules is called twice on
                    #        different model configurations, and PEFT employs different
                    #        module types depending on the chosen quantization.
                    module = cast(Linear, module)

                    # LoRA abliteration: delta W = -lambda * v * (v^T W)
                    # lora_B = -lambda * v
                    # lora_A = v^T W

                    # Use the FP32 refusal direction directly (no downcast/upcast)
                    # and move to the correct device.
                    v = layer_refusal_direction.to(module.weight.device)

                    # Get W (dequantize if necessary).
                    #
                    # FIXME: This cast is valid only under the assumption that the original
                    #        module wrapped by the LoRA adapter has a weight attribute.
                    #        See the comment above for why this is currently not guaranteed.
                    base_weight = cast(Tensor, module.base_layer.weight)
                    quant_state = getattr(base_weight, "quant_state", None)

                    if quant_state is None:
                        W = base_weight.to(torch.float32)
                    else:
                        # 4-bit quantization.
                        # This cast is always valid. Type inference fails here because the
                        # bnb.functional module is not found by ty for some reason.
                        W = cast(
                            Tensor,
                            bnb.functional.dequantize_4bit(  # ty:ignore[possibly-missing-attribute]
                                base_weight.data,
                                quant_state,
                            ).to(torch.float32),
                        )

                    # Flatten weight matrix to (out_features, in_features).
                    W = W.view(W.shape[0], -1)

                    if self.settings.row_normalization != RowNormalization.NONE:
                        # Keep a reference to the original weight matrix so we can subtract it later.
                        W_org = W
                        # Get the row norms.
                        W_row_norms = LA.vector_norm(W, dim=1, keepdim=True)
                        # Normalize the weight matrix along the rows.
                        W = F.normalize(W, p=2, dim=1)

                    # Calculate lora_A = v^T W
                    # v is (d_out,), W is (d_out, d_in)
                    # v @ W -> (d_in,)
                    lora_A = (v @ W).view(1, -1)

                    # Calculate lora_B = -weight * v
                    # v is (d_out,)
                    lora_B = (-weight * v).view(-1, 1)

                    if self.settings.row_normalization == RowNormalization.PRE:
                        # Make the LoRA adapter apply to the original weight matrix.
                        lora_B = W_row_norms * lora_B
                    elif self.settings.row_normalization == RowNormalization.FULL:
                        # Approximates https://huggingface.co/blog/grimjim/norm-preserving-biprojected-abliteration
                        W = W + lora_B @ lora_A
                        # Normalize the adjusted weight matrix along the rows.
                        W = F.normalize(W, p=2, dim=1)
                        # Restore the original row norms of the weight matrix.
                        W = W * W_row_norms
                        # Subtract the original matrix to turn W into a delta.
                        W = W - W_org
                        # Use a low-rank SVD to get an approximation of the matrix.
                        r = self.peft_config.r
                        U, S, Vh = torch.svd_lowrank(W, q=2 * r + 4, niter=6)
                        # Truncate it to the part we want to store in the LoRA adapter.
                        # Note: svd_lowrank actually returns V, so transpose it to get Vh.
                        U = U[:, :r]
                        S = S[:r]
                        Vh = Vh[:, :r].T
                        # Transfer it into the LoRA adapter components. Split the singular values
                        # evenly between the two components to keep their norms balanced and avoid
                        # potential issues with numerical stability.
                        sqrt_S = torch.sqrt(S)
                        lora_B = U @ torch.diag(sqrt_S)
                        lora_A = torch.diag(sqrt_S) @ Vh

                    # Skip modules whose base weight is on meta device (no actual data)
                    # or contains NaN values (corrupted or incompletely loaded weights).
                    if base_weight.device.type == "meta" or torch.isnan(W).any():
                        continue

                    # Assign to adapters. The adapter name is "default", because that's
                    # what PEFT uses when no name is explicitly specified, as above.
                    # These casts are therefore valid.
                    weight_A = cast(Tensor, module.lora_A["default"].weight)
                    weight_B = cast(Tensor, module.lora_B["default"].weight)
                    weight_A.data = lora_A.to(weight_A.dtype)
                    weight_B.data = lora_B.to(weight_B.dtype)

    def generate(
        self,
        prompts: list[Prompt],
        **kwargs: Any,
    ) -> tuple[BatchEncoding, GenerateDecoderOnlyOutput | LongTensor]:
        chats = [
            [
                {"role": "system", "content": prompt.system},
                {"role": "user", "content": prompt.user},
            ]
            for prompt in prompts
        ]

        # This cast is valid because list[str] is the return type
        # for batched operation with tokenize=False.
        chat_prompts = cast(
            list[str],
            self.tokenizer.apply_chat_template(
                chats,
                add_generation_prompt=True,
                tokenize=False,
            ),
        )

        if self.response_prefix:
            # Append the common response prefix to the prompts so that evaluation happens
            # at the point where responses start to differ for different prompts.
            chat_prompts = [prompt + self.response_prefix for prompt in chat_prompts]

        inputs = self.tokenizer(
            chat_prompts,
            return_tensors="pt",
            padding=True,
            return_token_type_ids=False,
        ).to(self.model.device)

        # FIXME: The type checker has been disabled here because of the extremely complex
        #        interplay between different generate() signatures and dynamic delegation.
        outputs = self.model.generate(
            **inputs,
            **kwargs,
            pad_token_id=self.tokenizer.pad_token_id,
            do_sample=False,  # Use greedy decoding to ensure deterministic outputs.
        )  # ty:ignore[call-non-callable]

        return inputs, outputs

    def get_responses(
        self,
        prompts: list[Prompt],
        skip_special_tokens: bool = False,
    ) -> list[str]:
        inputs, outputs = self.generate(
            prompts,
            max_new_tokens=self.settings.max_response_length,
        )

        return self.tokenizer.batch_decode(
            # Extract the newly generated part.
            # This cast is valid because the input_ids property is a Tensor
            # if the tokenizer is invoked with return_tensors="pt", as above.
            outputs[:, cast(Tensor, inputs["input_ids"]).shape[1] :],
            skip_special_tokens=skip_special_tokens,
        )

    def get_responses_batched(
        self,
        prompts: list[Prompt],
        skip_special_tokens: bool = False,
    ) -> list[str]:
        responses = []

        for batch in batchify(prompts, self.settings.batch_size):
            for response in self.get_responses(
                batch,
                skip_special_tokens=skip_special_tokens,
            ):
                responses.append(response)

        return responses

    def _get_hidden_states_via_hooks(self, inputs: BatchEncoding) -> list[Tensor]:
        """
        Capture per-layer hidden states using forward hooks.
        Used as a fallback for models (e.g. NemotronH) that don't return
        hidden_states through generate() or forward().

        Returns a list matching the standard output_hidden_states format:
        [embedding_output, layer_0_output, layer_1_output, ...] (n_layers + 1 entries).
        """
        captured: list[Tensor] = []

        def make_hook(idx: int):
            def hook(module: Module, args: Any, output: Any) -> None:
                # Layer output is typically a tuple where the first element
                # is the hidden state tensor of shape (batch, seq_len, hidden_dim).
                if isinstance(output, tuple):
                    captured.append(output[0].detach())
                else:
                    captured.append(output.detach())

            return hook

        # Also capture the input to the first layer (= embedding output)
        # to match the standard hidden_states format of n_layers + 1 entries.
        embedding_output: list[Tensor] = []

        def embedding_hook(module: Module, args: Any) -> None:
            # Pre-hooks receive (module, args). The first positional arg
            # to a layer is the hidden state input (= embedding output).
            if (
                isinstance(args, tuple)
                and len(args) > 0
                and isinstance(args[0], Tensor)
            ):
                embedding_output.append(args[0].detach())

        layers = self.get_layers()
        handles = []
        # Hook on the first layer to capture its input (= embedding output).
        handles.append(layers[0].register_forward_pre_hook(embedding_hook))
        for i, layer in enumerate(layers):
            handles.append(layer.register_forward_hook(make_hook(i)))

        try:
            self.model(**inputs)
        finally:
            for handle in handles:
                handle.remove()

        # Prepend embedding output to match [embedding, layer_0, layer_1, ...] format.
        if embedding_output:
            return [embedding_output[0]] + captured
        return captured

    def get_residuals(self, prompts: list[Prompt]) -> Tensor:
        # We only generate one token, and we return the residual vectors
        # at that token position, for each prompt and layer.
        inputs, outputs = self.generate(
            prompts,
            max_new_tokens=1,
            output_hidden_states=True,
            return_dict_in_generate=True,
        )

        # This cast is valid because GenerateDecoderOnlyOutput is the return type
        # of model.generate with return_dict_in_generate=True.
        outputs = cast(GenerateDecoderOnlyOutput, outputs)

        # Check if generate() returned usable hidden states.
        # Some models (e.g. NemotronH) return a tuple of Nones instead of actual tensors.
        has_hidden_states = (
            outputs.hidden_states is not None
            and len(outputs.hidden_states) > 0
            and outputs.hidden_states[0] is not None
        )

        if has_hidden_states:
            # Standard path: hidden states returned by generate().
            hidden_states = outputs.hidden_states[0]  # ty:ignore[non-subscriptable]
        else:
            # Fallback for hybrid architectures (e.g. NemotronH) that don't
            # return hidden_states through generate() or forward().
            # Use forward hooks to capture per-layer outputs directly.
            hidden_states = self._get_hidden_states_via_hooks(inputs)

        # The returned tensor has shape (prompt, layer, component).
        # Move all layer tensors to the same device before stacking,
        # since multi-GPU setups may place layers on different devices.
        target_device = hidden_states[0].device
        residuals = torch.stack(
            # layer_hidden_states has shape (prompt, position, component),
            # so this extracts the hidden states at the end of each prompt,
            # and stacks them up over the layers.
            [
                layer_hidden_states[:, -1, :].to(target_device)
                for layer_hidden_states in hidden_states
            ],
            dim=1,
        )

        # Upcast the data type to avoid precision (bfloat16) or range (float16)
        # problems during calculations involving residual vectors.
        residuals = residuals.to(torch.float32)

        # Warn about NaN residuals from hook-based capture on hybrid architectures.
        nan_layers = (
            torch.isnan(residuals).any(dim=(0, 2)).nonzero().squeeze(-1).tolist()
        )
        if nan_layers:
            print(f"  [bold yellow]Warning:[/] NaN residuals in layers: {nan_layers}")

        if 0 <= self.settings.winsorization_quantile < 1:
            # Apply symmetric winsorization to each layer of the per-prompt residuals.
            abs_residuals = torch.abs(residuals)
            # Get the (prompt, layer, 1) quantiles of the (prompt, layer, component) residuals.
            thresholds = torch.quantile(
                abs_residuals,
                self.settings.winsorization_quantile,
                dim=2,
                keepdim=True,
            )
            return torch.clamp(residuals, -thresholds, thresholds)

        return residuals

    def get_residuals_batched(self, prompts: list[Prompt]) -> Tensor:
        residuals = []

        for batch in batchify(prompts, self.settings.batch_size):
            residuals.append(self.get_residuals(batch))

        return torch.cat(residuals, dim=0)

    # We work with logprobs rather than probabilities for numerical stability
    # when computing the KL divergence.
    def get_logprobs(self, prompts: list[Prompt]) -> Tensor:
        # We only generate one token, and we return the (log) probability distributions
        # over the vocabulary at that token position, for each prompt.
        _, outputs = self.generate(
            prompts,
            max_new_tokens=1,
            output_scores=True,
            return_dict_in_generate=True,
        )

        # This cast is valid because GenerateDecoderOnlyOutput is the return type
        # of model.generate with return_dict_in_generate=True.
        outputs = cast(GenerateDecoderOnlyOutput, outputs)

        # Logits for the first (only) generated token.
        # This cast is valid because we passed output_scores=True above.
        assert outputs.scores is not None, (
            "Model did not return scores. This model architecture may not support output_scores."
        )
        logits = outputs.scores[0]

        if torch.isnan(logits).any():
            print(
                "  [bold yellow]Warning:[/] NaN values in logits (post-abliteration model corruption)"
            )

        # The returned tensor has shape (prompt, token).
        return F.log_softmax(logits, dim=-1)

    def get_logprobs_batched(self, prompts: list[Prompt]) -> Tensor:
        logprobs = []

        for batch in batchify(prompts, self.settings.batch_size):
            logprobs.append(self.get_logprobs(batch))

        return torch.cat(logprobs, dim=0)

    def stream_chat_response(self, chat: list[dict[str, str]]) -> str:
        # This cast is valid because str is the return type
        # for single-chat operation with tokenize=False.
        chat_prompt = cast(
            str,
            self.tokenizer.apply_chat_template(
                chat,
                add_generation_prompt=True,
                tokenize=False,
            ),
        )

        inputs = self.tokenizer(
            chat_prompt,
            return_tensors="pt",
            return_token_type_ids=False,
        ).to(self.model.device)

        streamer = TextStreamer(
            # The TextStreamer constructor annotates this parameter with the AutoTokenizer
            # type, which makes no sense because AutoTokenizer is a factory class,
            # not a base class that tokenizers inherit from.
            self.tokenizer,  # ty:ignore[invalid-argument-type]
            skip_prompt=True,
            skip_special_tokens=True,
        )

        # FIXME: The type checker has been disabled here because of the extremely complex
        #        interplay between different generate() signatures and dynamic delegation.
        outputs = self.model.generate(
            **inputs,
            streamer=streamer,
            max_new_tokens=4096,
        )  # ty:ignore[call-non-callable]

        return self.tokenizer.decode(
            outputs[0, inputs["input_ids"].shape[1] :],
            skip_special_tokens=True,
        )
